import os
import time
import logging
from typing import TypedDict, Optional
from langgraph.graph import StateGraph, END, START
from app.agents.tester import generate_tests
from app.agents.developer import generate_code
from app.agents.runner import run_pytest
from app.agents.reviewer import analyze_failures
from app.config import Config
from app.persistence import PersistenceStrategy, PersistenceFactory

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

class AgentState(TypedDict):
    specification: str
    tests: str
    code: str
    feedback: str
    status: str
    iteration: int
    test_phase: str  # 'red' or 'green'
    previous_tests: str  # Guardar testes anteriores para regeneraÃ§Ã£o de testes

class TDDOrchestrator:
    def __init__(
        self, 
        task_key: str = "tdd_task",
        persistence: Optional[PersistenceStrategy] = None
    ):
        """
        Inicializar o orquestrador TDD atravÃ©s de uma injeÃ§Ã£o de dependÃªncia.
        
        Args:
            task_key: Chave para armazenar o estado na camada de persistÃªncia.
            persistence: EstratÃ©gia de persistÃªncia a ser usada. Se None, cria persistÃªncia Redis padrÃ£o.
        """
        self.persistence = persistence or PersistenceFactory.create_persistence("redis")
        self.state_key = f"state:{task_key}"
        self.graph = self._build_graph()
        os.makedirs(Config.WORKSPACE_PATH, exist_ok=True)

    def _build_graph(self):
        def create_tests(state: AgentState) -> AgentState:
            iteration = state.get("iteration", 1)
            logging.info("=" * 60)
            if iteration == 1:
                logging.info("ğŸ“ FASE 1 (TDD): Gerando testes")
            else:
                logging.info(f"ğŸ“ REGENERANDO TESTES (iteraÃ§Ã£o {iteration})")
            logging.info("=" * 60)
            
            # Guardar testes anteriores antes de gerar novos
            previous_tests = state.get("tests", "")
            feedback = state.get("feedback", "")
            
            tests = generate_tests(state["specification"])
            
            # ValidaÃ§Ã£o
            if not tests:
                logging.error("âŒ Testes vazios gerados")
                raise ValueError("Falha ao gerar testes vÃ¡lidos")
            
            # Verificar se contÃ©m imports corretos
            module_name = Config.IMPLEMENTATION_MODULE
            if f"from {module_name} import" not in tests and f"import {module_name}" not in tests:
                logging.warning(f"âš ï¸ Testes nÃ£o importam de '{module_name}', corrigindo...")
                tests = f"from {module_name} import *\n\n" + tests
            
            # Verificar se nÃ£o hÃ¡ implementaÃ§Ã£o
            if "def " in tests and "def test_" not in tests:
                logging.warning("âš ï¸ Testes parecem conter implementaÃ§Ã£o, verificando...")
                non_test_funcs = []
                for line in tests.split('\n'):
                    stripped = line.strip()
                    if (stripped.startswith('def ') and 
                        'def test_' not in stripped and 
                        '@pytest.fixture' not in tests[max(0, tests.find(line)-100):tests.find(line)]):
                        non_test_funcs.append(stripped)
                
                if len(non_test_funcs) > 0:
                    logging.error(f"âŒ Testes contÃªm implementaÃ§Ã£o: {non_test_funcs}")
                    logging.error("Regenerando testes...")
                    tests = generate_tests(state["specification"])
            
            test_path = os.path.join(Config.WORKSPACE_PATH, Config.TEST_FILE)
            with open(test_path, "w", encoding="utf-8") as f:
                f.write(tests)
            
            logging.info(f"âœ… Testes salvos em: {Config.TEST_FILE}")
            logging.info(f"ğŸ“„ Preview:\n{tests[:400]}...")
            
            # Marcar que estamos na fase RED (sem implementaÃ§Ã£o ainda)
            return {"tests": tests, "test_phase": "red", "code": "", "previous_tests": previous_tests}

        def execute_tests_red(state: AgentState) -> AgentState:
            """Executa testes na fase RED - DEVE falhar pois nÃ£o hÃ¡ implementaÃ§Ã£o"""
            iteration = state.get("iteration", 1)
            logging.info("=" * 60)
            logging.info(f"ğŸ”´ FASE 2 (TDD - RED): Executando testes SEM implementaÃ§Ã£o (iteraÃ§Ã£o {iteration})")
            logging.info("=" * 60)
            logging.info("âš ï¸  Esperado: testes devem FALHAR (nÃ£o hÃ¡ cÃ³digo ainda)")
            
            # Criar arquivo vazio de implementaÃ§Ã£o para testes falharem corretamente
            impl_path = os.path.join(Config.WORKSPACE_PATH, f"{Config.IMPLEMENTATION_MODULE}.py")
            with open(impl_path, "w", encoding="utf-8") as f:
                f.write("# Arquivo vazio - implementaÃ§Ã£o virÃ¡ na fase GREEN\n")
            
            output = run_pytest()
            logging.info(f"ğŸ“Š Resultado pytest:\n{output}")
            
            # Verificar se falhou (como esperado no TDD)
            has_failures = "failed" in output.lower() or "error" in output.lower()
            
            if has_failures:
                logging.info("=" * 60)
                logging.info("âœ… RED confirmado: Testes falharam conforme esperado!")
                logging.info("=" * 60)
                # Analisar falhas para passar contexto ao Developer
                feedback = analyze_failures(output)
                return {"status": "red_confirmed", "feedback": feedback, "test_phase": "red"}
            else:
                logging.error("=" * 60)
                logging.error("âš ï¸  PROBLEMA: Testes passaram sem implementaÃ§Ã£o!")
                logging.error("âš ï¸  Isso indica que os testes podem estar incorretos.")
                logging.error("=" * 60)
                # Analisar por que os testes passaram sem cÃ³digo.
                feedback = analyze_failures(output + "\n\nAVISO: Testes passaram sem implementaÃ§Ã£o. Os testes podem nÃ£o estar validando corretamente a funcionalidade.")
                return {"status": "invalid_tests", "feedback": feedback, "test_phase": "red"}

        def create_code(state: AgentState) -> AgentState:
            iteration = state.get("iteration", 1)
            logging.info("=" * 60)
            if iteration == 1:
                logging.info("ğŸ’» FASE 3 (TDD - GREEN): Gerando cÃ³digo inicial")
            else:
                logging.info(f"ğŸ’» FASE 5 (TDD - REFACTOR): Refatorando cÃ³digo (iteraÃ§Ã£o {iteration})")
            logging.info("=" * 60)
            
            tests = state.get("tests", "")
            feedback = state.get("feedback", "")
            prev_code = state.get("code", "")
            
            code = generate_code(tests, feedback, prev_code)
            
            # ValidaÃ§Ã£o
            if not code:
                logging.error("âŒ CÃ³digo vazio gerado")
                raise ValueError("Falha ao gerar cÃ³digo vÃ¡lido")
            
            # Verificar se nÃ£o contÃ©m testes
            if "def test_" in code or "import pytest" in code:
                logging.warning("âš ï¸ CÃ³digo contÃ©m testes, removendo...")
                filtered_lines = []
                for line in code.split('\n'):
                    if ("def test_" not in line and 
                        "import pytest" not in line and 
                        ("assert " not in line or "# assert" in line)):
                        filtered_lines.append(line)
                code = "\n".join(filtered_lines)
            
            impl_path = os.path.join(Config.WORKSPACE_PATH, f"{Config.IMPLEMENTATION_MODULE}.py")
            with open(impl_path, "w", encoding="utf-8") as f:
                f.write(code)
            
            logging.info(f"âœ… CÃ³digo salvo em: {Config.IMPLEMENTATION_MODULE}.py")
            logging.info(f"ğŸ“„ Preview:\n{code[:400]}...")
            return {"code": code, "test_phase": "green"}

        def execute_tests_green(state: AgentState) -> AgentState:
            """Executa testes na fase GREEN - deve passar com a implementaÃ§Ã£o"""
            iteration = state.get("iteration", 1)
            logging.info("=" * 60)
            if iteration == 1:
                logging.info("ğŸ§ª FASE 4 (TDD - GREEN): Executando testes COM implementaÃ§Ã£o")
            else:
                logging.info(f"ğŸ§ª FASE 6 (TDD - REFACTOR CHECK): Verificando refatoraÃ§Ã£o (iteraÃ§Ã£o {iteration})")
            logging.info("=" * 60)
            logging.info("âœ… Esperado: testes devem PASSAR agora")
            
            output = run_pytest()
            logging.info(f"ğŸ“Š Resultado pytest:\n{output}")
            
            # Verificar se passou
            has_passed_tests = "passed" in output.lower()
            has_failures = "failed" in output.lower() or "error" in output.lower()
            
            # Sucesso = tem testes passando E nÃ£o tem falhas
            if has_passed_tests and not has_failures:
                logging.info("=" * 60)
                logging.info("âœ… âœ… âœ… GREEN: Todos os testes passaram! âœ… âœ… âœ…")
                logging.info("=" * 60)
                logging.info("ğŸ‰ Ciclo TDD completo: RED â†’ GREEN â†’ REFACTOR")
                return {"status": "passed", "feedback": ""}
            else:
                logging.warning("=" * 60)
                logging.warning("âŒ GREEN falhou: Testes ainda nÃ£o passam")
                logging.warning("=" * 60)
                feedback = analyze_failures(output)
                logging.info(f"ğŸ“‹ Feedback do Reviewer:\n{feedback}")
                return {"status": "failed", "feedback": feedback}

        def route_after_tests(state: AgentState) -> str:
            """Decide o prÃ³ximo passo apÃ³s criar os testes"""
            return "run_red_phase"

        def route_after_red(state: AgentState) -> str:
            """Decide o prÃ³ximo passo apÃ³s fase RED"""
            status = state.get("status")
            iteration = state.get("iteration", 0)
            
            # Verificar limite de iteraÃ§Ãµes
            if iteration >= Config.MAX_ITERATIONS:
                logging.error("=" * 60)
                logging.error("âš ï¸ âš ï¸ âš ï¸ LIMITE DE ITERAÃ‡Ã•ES ATINGIDO âš ï¸ âš ï¸ âš ï¸")
                logging.error("=" * 60)
                return "end"
            
            if status == "red_confirmed":
                # RED confirmado, pode gerar cÃ³digo
                return "generate_code"
            elif status == "invalid_tests":
                # Testes invÃ¡lidos (passaram sem cÃ³digo) - regenerate tests
                logging.warning(f"âš ï¸ Testes invÃ¡lidos detectados. Regenerando testes... (iteraÃ§Ã£o {iteration + 1}/{Config.MAX_ITERATIONS})")
                return "regenerate_tests"
            else:
                return "end"

        def route_after_green(state: AgentState) -> str:
            """Decide o prÃ³ximo passo apÃ³s fase GREEN"""
            status = state.get("status")
            
            if status == "passed":
                return "end"
            
            iteration = state.get("iteration", 0)
            if iteration >= Config.MAX_ITERATIONS:
                logging.error("=" * 60)
                logging.error("âš ï¸ âš ï¸ âš ï¸ LIMITE DE ITERAÃ‡Ã•ES ATINGIDO âš ï¸ âš ï¸ âš ï¸")
                logging.error("=" * 60)
                return "end"
            
            # Precisa refatorar
            return "refactor"

        # Construir o grafo de estados
        workflow = StateGraph(AgentState)
        
        # Adicionar nÃ³s
        workflow.add_node("create_tests", create_tests)
        workflow.add_node("run_red_phase", execute_tests_red)
        workflow.add_node("generate_code", create_code)
        workflow.add_node("run_green_phase", execute_tests_green)
        workflow.add_node("regenerate_tests", create_tests)  # Reuse create_tests for regeneration
        
        # Fluxo TDD: Tests â†’ RED â†’ Code â†’ GREEN â†’ (REFACTOR se falhar)
        workflow.add_edge(START, "create_tests")
        
        workflow.add_conditional_edges(
            "create_tests",
            route_after_tests,
            {
                "run_red_phase": "run_red_phase"
            }
        )
        
        workflow.add_conditional_edges(
            "run_red_phase",
            route_after_red,
            {
                "generate_code": "generate_code",
                "regenerate_tests": "regenerate_tests",
                "end": END
            }
        )
        
        # Depois de regenerar os testes, executar a fase RED novamente.
        workflow.add_edge("regenerate_tests", "run_red_phase")
        
        workflow.add_edge("generate_code", "run_green_phase")
        
        workflow.add_conditional_edges(
            "run_green_phase",
            route_after_green,
            {
                "end": END,
                "refactor": "generate_code"
            }
        )
        
        return workflow.compile()

    def run(self, specification: str):
        logging.info("ğŸš€ " * 20)
        logging.info("ğŸš€ INICIANDO WORKFLOW TDD COMPLETO")
        logging.info("ğŸš€ " * 20)
        logging.info(f"ğŸ“‹ EspecificaÃ§Ã£o:\n{specification}\n")
        logging.info("ğŸ“– Fluxo TDD: RED (falha) â†’ GREEN (passa) â†’ REFACTOR (melhora)")
        
        # Carregar ou inicializar estado.
        saved_state = self.persistence.load(self.state_key)
        
        initial_state: AgentState = {
            "specification": specification,
            "tests": saved_state.get("tests", ""),
            "code": saved_state.get("code", ""),
            "feedback": saved_state.get("feedback", ""),
            "status": saved_state.get("status", ""),
            "iteration": 0,
            "test_phase": "red",
            "previous_tests": ""
        }
        
        for i in range(Config.MAX_ITERATIONS):
            initial_state["iteration"] = i + 1
            
            # Executar o grafo
            final_state = None
            for state in self.graph.stream(initial_state):
                final_state = state
                if final_state:
                    node_name = list(state.keys())[0]
                    current_state = state[node_name]
                    self.persistence.save(self.state_key, current_state)
            
            # Atualizar o estado
            if final_state:
                node_name = list(final_state.keys())[0]
                initial_state.update(final_state[node_name])

            if initial_state.get("status") == "passed":
                break
            else:
                if i < Config.MAX_ITERATIONS - 1:
                    logging.info(f"â³ Aguardando 2s antes da iteraÃ§Ã£o {i + 2}...")
                    time.sleep(2)

        logging.info("\n" + "=" * 60)
        logging.info("ğŸ“Š RESULTADO FINAL DO TDD")
        logging.info("=" * 60)
        logging.info(f"âœ… Status: {initial_state.get('status', 'unknown')}")
        logging.info(f"ğŸ”¢ IteraÃ§Ãµes: {initial_state.get('iteration', 0)}")
        logging.info(f"ğŸ“„ ImplementaÃ§Ã£o: {Config.WORKSPACE_PATH}/{Config.IMPLEMENTATION_MODULE}.py")
        logging.info(f"ğŸ“„ Testes: {Config.WORKSPACE_PATH}/{Config.TEST_FILE}")
        
        if initial_state.get("status") == "passed":
            logging.info("ğŸ‰ Ciclo TDD concluÃ­do com sucesso!")
            logging.info("   âœ“ RED: Testes falharam inicialmente")
            logging.info("   âœ“ GREEN: ImplementaÃ§Ã£o passou todos os testes")
            logging.info("   âœ“ REFACTOR: CÃ³digo refinado (apenas se necessÃ¡rio)")
        
        logging.info("=" * 60)
        
        return initial_state