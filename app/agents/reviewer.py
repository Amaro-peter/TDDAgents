import re
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from app.config import Config

def extract_relevant_spec_context(
    specification: str,
    sub_requirement: str,
    test_output: str,
    current_code: str
) -> str:
    """
    Usa LLM para extrair APENAS a parte relevante da especificaÃ§Ã£o.
    Sem heurÃ­sticas frÃ¡geis - deixa a LLM decidir o que Ã© relevante.
    """
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)  # Modelo rÃ¡pido e barato
    
    system_msg = SystemMessage(content=(
        "VocÃª Ã© um assistente que extrai APENAS as informaÃ§Ãµes relevantes de uma especificaÃ§Ã£o.\n\n"
        "TAREFA:\n"
        "Dado:\n"
        "1. Uma especificaÃ§Ã£o completa\n"
        "2. Um sub-requisito atual\n"
        "3. SaÃ­da de um teste falhado\n"
        "4. CÃ³digo atual\n\n"
        "Extraia APENAS as regras/requisitos da especificaÃ§Ã£o que sÃ£o DIRETAMENTE RELEVANTES "
        "para resolver a falha do teste.\n\n"
        "REGRAS:\n"
        "- Se a falha envolve validaÃ§Ã£o, extraia as regras de validaÃ§Ã£o\n"
        "- Se a falha envolve cÃ¡lculo, extraia as regras de cÃ¡lculo\n"
        "- NÃƒO inclua informaÃ§Ãµes irrelevantes\n"
        "- Seja CIRÃšRGICO - apenas o necessÃ¡rio\n"
        "- Se nada Ã© relevante, retorne: 'Nenhum contexto especÃ­fico da especificaÃ§Ã£o Ã© necessÃ¡rio.'\n\n"
        "FORMATO DE RESPOSTA:\n"
        "Retorne apenas as regras extraÃ­das, sem comentÃ¡rios ou explicaÃ§Ãµes extras."
    ))
    
    human_msg = HumanMessage(content=(
        f"ğŸ“‹ ESPECIFICAÃ‡ÃƒO COMPLETA:\n{specification}\n\n"
        f"ğŸ¯ SUB-REQUISITO ATUAL:\n{sub_requirement}\n\n"
        f"ğŸ“Š SAÃDA DO TESTE (falha):\n{test_output}\n\n"
        f"ğŸ’» CÃ“DIGO ATUAL:\n```python\n{current_code}\n```\n\n"
        f"Extraia APENAS as regras da especificaÃ§Ã£o relevantes para corrigir esta falha."
    ))
    
    response = llm.invoke([system_msg, human_msg])
    return response.content.strip()


def analyze_failures(
    test_output: str,
    specification: str,
    sub_requirement: str,
    iteration: int = 0,
    max_retries: int = 3,
    current_code: str = "",
    test_code: str = ""
) -> str:
    """
    Analisa falhas com feedback GRADUAL usando LLM para filtragem.
    """
    llm = ChatOpenAI(model=Config.MODEL, temperature=0.3)

    # --- Extrai mÃ©tricas do pytest ---
    passed_match = re.search(r'(\d+)\s+passed', test_output)
    failed_match = re.search(r'(\d+)\s+failed', test_output)
    
    passed_count = int(passed_match.group(1)) if passed_match else 0
    failed_count = int(failed_match.group(1)) if failed_match else 0
    
    total = passed_count + failed_count
    
    # --- ESTRATÃ‰GIA DE FEEDBACK GRADUAL ---
    if iteration == 0:
        feedback_mode = "MINIMAL"
        spec_context = ""  # Sem contexto de spec
        
    elif iteration == 1:
        feedback_mode = "CONTEXTUAL"
        # âš ï¸ LLM extrai contexto relevante
        spec_context = extract_relevant_spec_context(
            specification=specification,
            sub_requirement=sub_requirement,
            test_output=test_output,
            current_code=current_code
        )
        
    else:  # iteration >= 2
        feedback_mode = "ARCHITECTURAL"
        spec_context = specification  # Spec completa para anÃ¡lise profunda

    # --- SYSTEM MESSAGE (instruÃ§Ãµes de comportamento) ---
    system_msg = SystemMessage(content=(
        f"VocÃª Ã© um REVISOR SÃŠNIOR de TDD.\n\n"
        f"ğŸ¯ MODO ATUAL: {feedback_mode} (iteraÃ§Ã£o {iteration}/{max_retries})\n\n"
        f"ğŸ“‹ PRINCÃPIO DE FEEDBACK GRADUAL:\n\n"
        f"MINIMAL (iteraÃ§Ã£o 0):\n"
        f"- Identifique APENAS o que estÃ¡ falhando\n"
        f"- NÃƒO explique regras da especificaÃ§Ã£o\n"
        f"- NÃƒO sugira implementaÃ§Ãµes completas\n"
        f"- Formato: 'O teste espera X mas recebe Y. Analise o motivo.'\n\n"
        f"CONTEXTUAL (iteraÃ§Ã£o 1):\n"
        f"- VocÃª receberÃ¡ APENAS a parte RELEVANTE da especificaÃ§Ã£o\n"
        f"- Explique POR QUE o teste espera aquele resultado\n"
        f"- DÃª dica de ONDE implementar (nÃ£o o cÃ³digo completo)\n"
        f"- Formato: 'Segundo a spec: [regra]. Adicione validaÃ§Ã£o em [local].'\n\n"
        f"ARCHITECTURAL (iteraÃ§Ã£o 2+):\n"
        f"- VocÃª receberÃ¡ a especificaÃ§Ã£o COMPLETA\n"
        f"- Analise se hÃ¡ conflito de requisitos\n"
        f"- Sugira mudanÃ§a de abordagem se necessÃ¡rio\n"
        f"- Considere redesign da arquitetura\n\n"
        f"âš ï¸ REGRA CRÃTICA EM TODOS OS MODOS:\n"
        f"NUNCA sugira implementar recursos alÃ©m do teste atual.\n"
        f"Foque APENAS em fazer o teste falhado passar.\n"
        f"O TDD Ã© incremental - uma falha por vez."
    ))

    # --- HUMAN MESSAGE (contexto especÃ­fico por modo) ---
    if feedback_mode == "MINIMAL":
        human_msg = HumanMessage(content=(
            f"ğŸ¯ SUB-REQUISITO: {sub_requirement}\n\n"
            f"ğŸ“Š SITUAÃ‡ÃƒO:\n"
            f"- Testes passados: {passed_count}\n"
            f"- Testes falhados: {failed_count}\n\n"
            f"ğŸ“Š SAÃDA PYTEST:\n```\n{test_output}\n```\n\n"
            f"TAREFA (MINIMAL):\n"
            f"Identifique o erro de forma direta e objetiva.\n"
            f"NÃƒO explique regras ou dÃª soluÃ§Ãµes completas.\n"
            f"Apenas: 'O teste espera X mas retorna Y.'"
        ))
        
    elif feedback_mode == "CONTEXTUAL":
        human_msg = HumanMessage(content=(
            f"ğŸ¯ SUB-REQUISITO: {sub_requirement}\n\n"
            f"ğŸ“Š SITUAÃ‡ÃƒO:\n"
            f"- Testes passados: {passed_count}\n"
            f"- Testes falhados: {failed_count}\n"
            f"- Tentativa: {iteration + 1}\n\n"
            f"ğŸ“‹ CONTEXTO RELEVANTE DA ESPECIFICAÃ‡ÃƒO:\n{spec_context}\n\n"
            f"ğŸ’» CÃ“DIGO ATUAL:\n```python\n{current_code}\n```\n\n"
            f"ğŸ“Š SAÃDA PYTEST:\n```\n{test_output}\n```\n\n"
            f"TAREFA (CONTEXTUAL):\n"
            f"Use o contexto da especificaÃ§Ã£o para explicar POR QUE o teste falha.\n"
            f"DÃª dica de ONDE implementar a correÃ§Ã£o (nÃ£o o cÃ³digo completo).\n"
            f"Mantenha o foco APENAS no teste atual."
        ))
        
    else:  # ARCHITECTURAL
        human_msg = HumanMessage(content=(
            f"ğŸ¯ SUB-REQUISITO: {sub_requirement}\n\n"
            f"ğŸ“Š SITUAÃ‡ÃƒO CRÃTICA:\n"
            f"- Testes passados: {passed_count}\n"
            f"- Testes falhados: {failed_count}\n"
            f"- Tentativa: {iteration + 1}/{max_retries}\n"
            f"- âš ï¸ MÃºltiplas falhas no mesmo teste\n\n"
            f"ğŸ“‹ ESPECIFICAÃ‡ÃƒO COMPLETA:\n{specification}\n\n"
            f"ğŸ’» CÃ“DIGO ATUAL:\n```python\n{current_code}\n```\n\n"
            f"ğŸ“‹ TODOS OS TESTES:\n```python\n{test_code}\n```\n\n"
            f"ğŸ“Š SAÃDA PYTEST:\n```\n{test_output}\n```\n\n"
            f"TAREFA (ARCHITECTURAL):\n"
            f"1. Analise se hÃ¡ conflito entre testes ou requisitos\n"
            f"2. Identifique se a arquitetura atual pode resolver o problema\n"
            f"3. Sugira redesign se necessÃ¡rio\n"
            f"4. Se nÃ£o houver conflito, identifique o erro de implementaÃ§Ã£o especÃ­fico"
        ))

    response = llm.invoke([system_msg, human_msg])
    return response.content.strip()